# catalyst_watch.py
import os, json, time, logging, math, requests
from datetime import datetime, timedelta, timezone
from typing import Iterable, Set, List, Tuple
from bs4 import BeautifulSoup
import yfinance as yf
from google.cloud import storage
from dotenv import load_dotenv

load_dotenv()
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")

# ==== ENV ====
PRICE_MIN = float(os.getenv("CAT_PRICE_MIN", "3.0"))
PRICE_MAX = float(os.getenv("CAT_PRICE_MAX", "15.0"))
VOL_SPIKE_MULT = float(os.getenv("CAT_VOL_SPIKE_MULT", "3.0"))   # –≤–æ —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –≤—ã—à–µ —Å—Ä–µ–¥–Ω–µ–≥–æ 10–¥–Ω
PRICE_MOVE_PCT = float(os.getenv("CAT_PRICE_MOVE_PCT", "8.0"))   # % –∑–∞ –¥–µ–Ω—å
NEWS_MIN = int(os.getenv("CAT_NEWS_MIN", "1"))                   # –º–∏–Ω. —á–∏—Å–ª–æ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π/—Ñ–∞–π–ª–∏–Ω–≥–æ–≤
YF_SLEEP = float(os.getenv("CAT_YF_SLEEP", "0.6"))
SEC_DAYS_BACK = int(os.getenv("SEC_DAYS_BACK", "14"))            # –≥–ª—É–±–∏–Ω–∞ –¥–ª—è 10-12B

GCS_BUCKET = os.getenv("GCS_BUCKET")
CAT_WATCHLIST_GCS_BLOB = os.getenv("CAT_WATCHLIST_GCS_BLOB", "catalyst/watchlist.txt")

TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID")

# ==== GCS ====
_gcs = None
def _get_gcs():
    global _gcs
    if _gcs: return _gcs
    key_str = os.getenv("GCS_KEY_JSON") or os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
    if not key_str:
        raise ValueError("‚ùå –ù–µ—Ç –∫–ª—é—á–∞ GCS (GCS_KEY_JSON –∏–ª–∏ GOOGLE_APPLICATION_CREDENTIALS)")
    with open("gcs_key.json", "w", encoding="utf-8") as f:
        json.dump(json.loads(key_str), f)
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "gcs_key.json"
    _gcs = storage.Client()
    return _gcs

def gcs_save_text(bucket: str, key: str, text: str):
    if not bucket: return
    _get_gcs().bucket(bucket).blob(key).upload_from_string(text, content_type="text/plain; charset=utf-8")
    logging.info(f"Saved to gs://{bucket}/{key} ({len(text)} bytes)")

def gcs_load_text(bucket: str, key: str, default: str = "") -> str:
    try:
        blob = _get_gcs().bucket(bucket).blob(key)
        return blob.download_as_text(encoding="utf-8") if blob.exists() else default
    except Exception:
        return default

# ==== Telegram ====
def tg_send(text: str):
    if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHAT_ID:
        logging.info("[TG MOCK] " + text)
        return
    try:
        requests.post(
            f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage",
            json={"chat_id": TELEGRAM_CHAT_ID, "text": text, "parse_mode": "HTML"},
            timeout=20,
        )
    except Exception as e:
        logging.warning(f"TG send error: {e}")

# ==== –ò—Å—Ç–æ—á–Ω–∏–∫ 1: IPO (Nasdaq) ====
def fetch_nasdaq_ipos() -> Set[str]:
    """
    –¢—è–Ω–µ–º —Ç–∞–±–ª–∏—Ü—ã —Å https://www.nasdaq.com/market-activity/ipos (recent/upcoming).
    –ë–µ—Ä—ë–º –ø–µ—Ä–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü (—Ç–∏–∫–µ—Ä). –°–∞–π—Ç –∏–Ω–æ–≥–¥–∞ –º–µ–Ω—è–µ—Ç –≤—ë—Ä—Å—Ç–∫—É ‚Äî –¥–µ—Ä–∂–∏–º –ø–∞—Ä—Å–∏–Ω–≥ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —É—Å—Ç–æ–π—á–∏–≤—ã–º.
    """
    url = "https://www.nasdaq.com/market-activity/ipos"
    r = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=30)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, "html.parser")
    tickers: Set[str] = set()

    # –õ—é–±–∞—è —Ç–∞–±–ª–∏—Ü–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: –∏—â–µ–º —è—á–µ–π–∫–∏, –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ —Ç–∏–∫–µ—Ä
    for table in soup.find_all("table"):
        for row in table.find_all("tr"):
            cols = row.find_all("td")
            if not cols: continue
            t = cols[0].get_text(strip=True).upper()
            # —Ç–∏–∫–µ—Ä: 1-5 –ª–∞—Ç–∏–Ω—Å–∫–∏—Ö –±—É–∫–≤ (–¥–æ–ø—É—Å–∫–∞–µ–º .U/.W –¥–ª—è —é–Ω–∏—Ç–æ–≤/–≤–∞—Ä—Ä–∞–Ω—Ç–æ–≤, –Ω–æ –æ–±—Ä–µ–∂–µ–º –¥–æ –±–∞–∑—ã)
            base = t.replace(".U", "").replace(".W", "")
            if 1 <= len(base) <= 5 and base.isalpha():
                tickers.add(base)
    logging.info(f"Nasdaq IPO: {len(tickers)}")
    return tickers

# ==== –ò—Å—Ç–æ—á–Ω–∏–∫ 2: Spin-offs (SEC 10-12B) ====
def fetch_sec_spinoffs(days_back: int = 14) -> Set[str]:
    """
    –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π SEC search-index (–±–µ–∑ —Å—Ç–æ—Ä–æ–Ω–Ω–∏—Ö —Å–µ—Ä–≤–∏—Å–æ–≤).
    –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ñ–æ—Ä–º—ã 10-12B –∑–∞ days_back –¥–Ω–µ–π –∏ —Å–æ–±–∏—Ä–∞–µ–º —Ç–∏–∫–µ—Ä—ã.
    """
    # API –∏–Ω–¥–µ–∫—Å–∞: https://efts.sec.gov/LATEST/search-index
    # –û–≥—Ä–∞–Ω–∏—á–∏–º —Ä–∞–∑–º–µ—Ä, —á—Ç–æ–±—ã –Ω–µ –∑–ª–æ—É–ø–æ—Ç—Ä–µ–±–ª—è—Ç—å.
    since = (datetime.utcnow() - timedelta(days=days_back)).strftime("%Y-%m-%d")
    body = {
        "query": {"query_string": {"query": f'formType:"10-12B" AND filedAt:>={since}'}},
        "from": 0,
        "size": 200,
        "sort": [{"filedAt": {"order": "desc"}}],
        "highlight": False,
    }
    r = requests.post("https://efts.sec.gov/LATEST/search-index", json=body, timeout=30,
                      headers={"User-Agent":"Mozilla/5.0"})
    if r.status_code != 200:
        logging.warning(f"SEC 10-12B request failed: {r.status_code} {r.text[:200]}")
        return set()
    data = r.json()
    hits = data.get("hits", {}).get("hits", [])
    res: Set[str] = set()
    for h in hits:
        tick = (h.get("_source", {}).get("ticker") or "").upper().strip()
        if tick and tick != "N/A":
            base = tick.replace(".U","").replace(".W","")
            if 1 <= len(base) <= 5 and base.isalpha():
                res.add(base)
    logging.info(f"SEC 10-12B (spin-offs): {len(res)}")
    return res

# ==== –ú–µ—Ç—Ä–∏–∫–∏ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ ====
def get_hist(ticker: str):
    t = yf.Ticker(ticker)
    hist = t.history(period="15d", interval="1d", auto_adjust=False)
    return hist if hist is not None and not hist.empty else None

def volume_spike(hist) -> Tuple[bool, float, float]:
    """
    –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –æ–±—ä—ë–º —Å SMA(10) –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –¥–Ω–µ–π.
    """
    vol = hist["Volume"].dropna()
    if len(vol) < 12:  # –Ω—É–∂–µ–Ω —Ö–æ—Ç—è –±—ã ~10 –¥–Ω–µ–π –∏—Å—Ç–æ—Ä–∏–∏ + —Ç–µ–∫—É—â–∏–π
        return (False, 0.0, 0.0)
    last = float(vol.iloc[-1])
    base = float(vol.iloc[-11:-1].mean())
    if base <= 0:
        return (False, last, base)
    return (last >= VOL_SPIKE_MULT * base, last, base)

def price_move(hist) -> Tuple[bool, float]:
    """
    % –∏–∑–º–µ–Ω–µ–Ω–∏—è: –ø–æ—Å–ª–µ–¥–Ω–∏–π Close vs –ø—Ä–µ–¥—ã–¥—É—â–∏–π Close.
    """
    close = hist["Close"].dropna()
    if len(close) < 2:
        return (False, 0.0)
    pct = (float(close.iloc[-1]) / float(close.iloc[-2]) - 1.0) * 100.0
    return (pct >= PRICE_MOVE_PCT, pct)

def in_price_range(hist) -> Tuple[bool, float]:
    last_close = float(hist["Close"].dropna().iloc[-1])
    return (PRICE_MIN <= last_close <= PRICE_MAX, last_close)

def recent_news_count_yf(ticker: str, hours: int = 24) -> int:
    """
    –ü—ã—Ç–∞–µ–º—Å—è –≤–∑—è—Ç—å —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ yfinance (–µ—Å–ª–∏ –µ—Å—Ç—å).
    –°—á–∏—Ç–∞–µ–º –∑–∞–ø–∏—Å–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 'hours' —á–∞—Å–æ–≤.
    """
    try:
        t = yf.Ticker(ticker)
        # yfinance>=0.2.40: t.news -> —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å providerPublishTime (unix)
        news = getattr(t, "news", []) or []
        if not isinstance(news, list):
            return 0
        cutoff = datetime.now(timezone.utc) - timedelta(hours=hours)
        cnt = 0
        for n in news:
            ts = n.get("providerPublishTime")
            if ts is None:  # –∏–Ω–æ–≥–¥–∞ –ø–æ–ª–µ –¥—Ä—É–≥–æ–µ
                ts = n.get("providerPublishTimeUtc")
            if ts is None:
                continue
            dt = datetime.fromtimestamp(int(ts), tz=timezone.utc)
            if dt >= cutoff:
                cnt += 1
        return cnt
    except Exception:
        return 0

def recent_filings_count_sec(ticker: str, hours: int = 24) -> int:
    """
    –°—á–∏—Ç–∞–µ–º —Å–≤–µ–∂–∏–µ 8-K/PR-–ø–æ—Ö–æ–∂–∏–µ —Ñ–∞–π–ª–∏–Ω–≥–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 'hours' —á–∞—Å–æ–≤ —á–µ—Ä–µ–∑ SEC search-index.
    """
    cutoff = (datetime.utcnow() - timedelta(hours=hours)).strftime("%Y-%m-%dT%H:%M:%S")
    q = {
        "query": {"query_string": {"query": f'ticker:"{ticker}" AND filedAt:>={cutoff} AND (formType:"8-K" OR formType:"6-K")'}},
        "from": 0, "size": 50,
        "sort": [{"filedAt": {"order": "desc"}}],
        "highlight": False,
    }
    try:
        r = requests.post("https://efts.sec.gov/LATEST/search-index", json=q, timeout=25,
                          headers={"User-Agent":"Mozilla/5.0"})
        if r.status_code != 200:
            return 0
        data = r.json()
        return int(data.get("hits", {}).get("total", {}).get("value", 0))
    except Exception:
        return 0

# ==== –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ ====
def analyze_ticker(ticker: str):
    hist = get_hist(ticker)
    if hist is None: return None

    in_rng, last_px = in_price_range(hist)
    if not in_rng:
        return None

    v_spike, v_last, v_base = volume_spike(hist)
    p_move, pct = price_move(hist)

    news_yf = recent_news_count_yf(ticker, hours=24)
    filings = recent_filings_count_sec(ticker, hours=24)
    news_total = news_yf + filings
    news_hit = news_total >= NEWS_MIN

    hit = v_spike or p_move or news_hit
    if not hit:
        return None

    return {
        "ticker": ticker,
        "price": round(last_px, 2),
        "vol_spike": v_spike,
        "vol_last": int(v_last),
        "vol_base": int(v_base),
        "price_move_pct": round(pct, 2),
        "news_24h": int(news_total),
        "news_yf": int(news_yf),
        "filings_24h": int(filings),
    }

def fetch_watchlist() -> List[str]:
    # –°–æ–±–∏—Ä–∞–µ–º —Å–≤–µ–∂–∏–π —Å–ø–∏—Å–æ–∫: IPO (Nasdaq) + —Å–ø–∏–Ω-–æ—Ñ—Ñ—ã (SEC 10-12B)
    ipos = fetch_nasdaq_ipos()
    spins = fetch_sec_spinoffs(days_back=SEC_DAYS_BACK)
    all_tick = sorted(ipos.union(spins))
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º ¬´–∏—Å—Ç–æ—á–Ω–∏–∫ –∏—Å—Ç–∏–Ω—ã¬ª –≤ GCS
    if GCS_BUCKET and all_tick:
        gcs_save_text(GCS_BUCKET, CAT_WATCHLIST_GCS_BLOB, "\n".join(all_tick))
    # –ï—Å–ª–∏ –≤–¥—Ä—É–≥ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —É–ø–∞–ª–∞ ‚Äî –±–µ—Ä—ë–º –ø—Ä–æ—à–ª—ã–π —Å–ø–∏—Å–æ–∫ –∏–∑ GCS
    if not all_tick and GCS_BUCKET:
        cached = gcs_load_text(GCS_BUCKET, CAT_WATCHLIST_GCS_BLOB, "")
        all_tick = [t.strip().upper() for t in cached.splitlines() if t.strip()]
    return all_tick

def main():
    watch = fetch_watchlist()
    logging.info(f"Watchlist size: {len(watch)}")

    hits = []
    for t in watch:
        try:
            res = analyze_ticker(t)
            if res:
                hits.append(res)
            time.sleep(YF_SLEEP)
        except Exception as e:
            logging.info(f"{t}: skip ({e})")

    if not hits:
        logging.info("No catalysts today in range.")
        return

    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫—Ä–∞—Ç–∫–æ–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –∏ –ø–æ–¥—Ä–æ–±–Ω—ã–π —Ö–≤–æ—Å—Ç
    lines = []
    for h in hits[:60]:
        flags = []
        if h["vol_spike"]: flags.append(f"Vol‚â•{VOL_SPIKE_MULT}√ó ({h['vol_last']}/{max(1,h['vol_base'])})")
        if abs(h["price_move_pct"]) >= PRICE_MOVE_PCT: flags.append(f"Œî{h['price_move_pct']}%")
        if h["news_24h"] >= NEWS_MIN: flags.append(f"News:{h['news_24h']} (YF {h['news_yf']}/SEC {h['filings_24h']})")
        lines.append(f"{h['ticker']} (${h['price']}) ‚Äî " + ", ".join(flags) if flags else f"{h['ticker']} (${h['price']})")

    msg = (
        f"üöÄ <b>Catalyst Watch</b> ${PRICE_MIN:.0f}‚Äì${PRICE_MAX:.0f}\n"
        f"–¢—Ä–∏–≥–≥–µ—Ä—ã: Vol‚â•{VOL_SPIKE_MULT}√ó, |Œî|‚â•{PRICE_MOVE_PCT}%, News‚â•{NEWS_MIN}\n"
        + "\n".join(lines)
    )
    tg_send(msg)
    logging.info(f"Alerts: {len(hits)} tickers")

if __name__ == "__main__":
    main()
